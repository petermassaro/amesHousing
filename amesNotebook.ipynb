{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ames Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from amesHousing import cardinality_check, segment_features\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames = pd.read_csv('Data/train.csv')\n",
    "ames.drop('Id', axis=1, inplace=True)\n",
    "ames_target = ames.pop('SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've imported a function called cardinality_check that I defined to create a summary table of data types and unique values. The segment_features function will use this dataframe to separate features into numeric, categorical and ordinal - and for now, I'll group ordinal variables with categorical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_unique</th>\n",
       "      <th>pct_unique</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LotArea</th>\n",
       "      <td>1073</td>\n",
       "      <td>0.734932</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GrLivArea</th>\n",
       "      <td>861</td>\n",
       "      <td>0.589726</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <td>780</td>\n",
       "      <td>0.534247</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1stFlrSF</th>\n",
       "      <td>753</td>\n",
       "      <td>0.515753</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <td>721</td>\n",
       "      <td>0.493836</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             n_unique  pct_unique data_type\n",
       "LotArea          1073    0.734932     int64\n",
       "GrLivArea         861    0.589726     int64\n",
       "BsmtUnfSF         780    0.534247     int64\n",
       "1stFlrSF          753    0.515753     int64\n",
       "TotalBsmtSF       721    0.493836     int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_check = cardinality_check(ames)\n",
    "data_check.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features with an \"object\" data type will be classified as categorical.  For int or float variables that are actually categorical, I defined a cutoff in terms of percent of unique values for that feature - ie features with a low number of distinct values will be classified as ordinal and treated as categorical for this stage of the analysis.  I went with a rather arbitrary cutoff of 1.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = segment_features(ames, .015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I define a pipeline to prep the features for a model.  Here, I'll take the extra step of trying to recover feature names from several of sklearn's transformers, which can be a bit of a process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transform = Pipeline(steps=[\n",
    "        ('impute', IterativeImputer(add_indicator=True))\n",
    "        ])\n",
    "    \n",
    "categorical_transform = Pipeline(steps=[\n",
    "        ('impute', SimpleImputer(strategy='most_frequent', add_indicator=True)),\n",
    "        ('one_hot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "   \n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "        ('numeric', numeric_transform, features['num']),\n",
    "        ('categorical', categorical_transform, np.concatenate(\n",
    "                (features['cat'], features['ord']),\n",
    "                )\n",
    "    )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessor.fit_transform(ames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8 14 15] [ 9 13 14 19 20 22 24 25 27 28 30 31 32 37 38 41]\n"
     ]
    }
   ],
   "source": [
    "num_missing = preprocessor.named_transformers_['numeric'].named_steps['impute'].indicator_\n",
    "cat_missing = preprocessor.named_transformers_['categorical'].named_steps['impute'].indicator_\n",
    "print(num_missing.features_, cat_missing.features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit gives feature lists for their missing indicator method by index.  As a sanity check, I'll take a look at the indices returned and cross reference with the original data set to make sure everything is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LotArea            0\n",
       "GrLivArea          0\n",
       "BsmtUnfSF          0\n",
       "1stFlrSF           0\n",
       "TotalBsmtSF        0\n",
       "BsmtFinSF1         0\n",
       "GarageArea         0\n",
       "2ndFlrSF           0\n",
       "MasVnrArea         8\n",
       "WoodDeckSF         0\n",
       "OpenPorchSF        0\n",
       "BsmtFinSF2         0\n",
       "EnclosedPorch      0\n",
       "YearBuilt          0\n",
       "LotFrontage      259\n",
       "GarageYrBlt       81\n",
       "ScreenPorch        0\n",
       "YearRemodAdd       0\n",
       "LowQualFinSF       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_feature_df = ames[features['num']]\n",
    "num_feature_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MasVnrArea', 'LotFrontage', 'GarageYrBlt'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['num'][num_missing.features_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the missing indicator output appears to tie out with features missing data. \n",
    "\n",
    "According to scikit's documentation, the \"add_indicator\" keyword arg for its imputer functions will \"stack on\" to the output of that transformer, so for numeric variables, we should get three indicator columns appended to the numeric data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
